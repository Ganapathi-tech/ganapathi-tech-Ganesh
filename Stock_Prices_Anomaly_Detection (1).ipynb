{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFBl3DsqB3AE"
      },
      "source": [
        "\n",
        "\n",
        "# Stock Prices Anomaly Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-kxaHhwXEp9"
      },
      "source": [
        "**DISCLAIMER:** THIS NOTEBOOK IS PROVIDED ONLY AS A REFERENCE SOLUTION NOTEBOOK FOR THE MINI-PROJECT. THERE MAY BE OTHER POSSIBLE APPROACHES/METHODS TO ACHIEVE THE SAME RESULTS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "##  Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95F1ym6qB8VU"
      },
      "source": [
        "\n",
        "\n",
        "* perform PCA based stock analytics\n",
        "* analyze and create time series data\n",
        "* implement LSTM auto-encoders\n",
        "* detect the anomalies based on the loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8aczZmzvXTc"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N04YT8wkBV8"
      },
      "source": [
        "Autoencoder Neural Networks try to learn data representation of its input. Usually, we want to learn an efficient encoding that uses fewer parameters/memory. The encoding should allow for output similar to the original input. In a sense, weâ€™re forcing the model to learn the most important features of the data using as few parameters as possible.\n",
        "\n",
        "LSTM autoencoder is an encoder that makes use of LSTM encoder-decoder architecture to compress data using an encoder and decode it to retain original structure using a decoder.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A3CeBTlkclZ"
      },
      "source": [
        "**Anomaly Detection**\n",
        "\n",
        "Anomaly detection refers to the task of finding/identifying rare events/data points. Some applications include - bank fraud detection, tumor detection in medical imaging, and errors in written text.\n",
        "\n",
        "A lot of supervised and unsupervised approaches for anomaly detection have been proposed. Some of the approaches include - One-class SVMs, Bayesian Networks, Cluster analysis, and Neural Networks.\n",
        "\n",
        "We will use an LSTM Autoencoder Neural Network to detect/predict anomalies (sudden price changes) in the S&P 500 index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgx1PkHfCDyJ"
      },
      "source": [
        "## Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmPsF-eygjVm"
      },
      "source": [
        "This mini-project consists of two parts and two different stock price datasets:\n",
        "\n",
        "### PART A\n",
        "\n",
        "Using the **S&P 500 stock prices data of different companies**, we will perform a PCA based analysis.\n",
        "\n",
        "### PART B\n",
        "\n",
        "Using the **S&P 500 stock price index time series data**, we will perform anomaly detection in the stock prices across the years. The dataset chosen is is S&P500 Daily Index a .csv format with one column with a daily timestamp and the second column with the raw, un-adjusted closing prices for each day. This long term, granular time series dataset allows researchers to have a good sized publicly available financial dataset to explore time series trends or use as part of a quantitative finance project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HDD5ANilAFk"
      },
      "source": [
        "Detect the stock price anomalies by implementing an LSTM autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82-MCqP0yFM2"
      },
      "source": [
        "#@title Download dataset\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/SPY.csv\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/prices.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOD5BSv-zts3"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import Conv2D, Conv3D, UpSampling2D\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten\n",
        "from keras.layers import LSTM, RepeatVector, TimeDistributed\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.models import Sequential, Model\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNqIbfq5fD7-"
      },
      "source": [
        "## PCA Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D3vSPaQ4D7x"
      },
      "source": [
        "Principal Component Analysis (PCA) decomposes the data into many vectors called principal components. These summaries are linear combinations of the input features that try to explain as much variance in the data as possible. By convention, these principal components are ordered by the amount of variance they can explain, with the first principal component explaining most of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdTLbP3MpFec"
      },
      "source": [
        "Perform PCA based analytics on the stock prices data from different companies.\n",
        "\n",
        "Hint: Refer to the article [here](https://towardsdatascience.com/stock-market-analytics-with-pca-d1c2318e3f0e)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAQhjOKMPXSO"
      },
      "source": [
        "### Load and pre-process the prices data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Red8vqbEmjRk"
      },
      "source": [
        "prices = pd.read_csv(\"prices.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Mjhd_KmuzI"
      },
      "source": [
        "prices.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9env0vqmzQ5"
      },
      "source": [
        "prices.fillna(0,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN0jFdXq3jri"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(prices.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V0Oc8g6zR3I"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_prices = scaler.fit_transform(prices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGfxFqC2Pfv6"
      },
      "source": [
        "### Apply PCA\n",
        "\n",
        "* plot the explained variance ratio. Hint: `pca.explained_variance_ratio_`\n",
        "* Represent the components which preserve maximum information and plot to visualize\n",
        "* Compute the daily returns of the 500 company stocks. Hint: See the following [reference](https://towardsdatascience.com/stock-market-analytics-with-pca-d1c2318e3f0e).\n",
        "* Plot the stocks with most negative and least negative PCA weights in the pandemic period (Year 2020). Use reference as above. Discuss the least and most impacted industrial sectors in terms of stocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjsVhditN5Qk"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(scaled_prices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBWPh0d8KNaA"
      },
      "source": [
        "table = [prices.columns.values[1:],pca.components_[0]]\n",
        "pd.DataFrame(pca.components_.T,columns=['component '+str(i+1) for i in range(394)]\n",
        "             ,index=prices.columns.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr-26wOaQz79"
      },
      "source": [
        "plt.bar(range(len(pca.explained_variance_ratio_[:20])), pca.explained_variance_ratio_[:20])\n",
        "plt.xlabel('components')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BNY78Dnm6TN"
      },
      "source": [
        "pc1 = pd.Series(index=prices.columns, data=pca.components_[0])\n",
        "pc1.plot(figsize=(10,6), xticks=[], grid=True, title='First Principal Component of the S&P500')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWVKuA1t2ibj"
      },
      "source": [
        "fig, ax = plt.subplots(2,1, figsize=(24,20))\n",
        "pc1.nsmallest(10).plot.bar(ax=ax[0], color='green', grid=True, title='Stocks with Most Negative PCA Weights')\n",
        "pc1.nlargest(10).plot.bar(ax=ax[1], color='blue', grid=True, title='Stocks with Least Negative PCA Weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR9EtC0WPyiW"
      },
      "source": [
        "#### Apply T-SNE and visualize with a graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhIhWG-L3Q_2"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components = 2, random_state = 0)\n",
        "tsne_data = tsne.fit_transform(prices)\n",
        "print(tsne_data.shape)\n",
        "plt.scatter(tsne_data[:,0],tsne_data[:,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AjTe8ZRO_QD"
      },
      "source": [
        "## Anomaly Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtKhlUM9fTbI"
      },
      "source": [
        "### Load and Preprocess the data\n",
        "\n",
        "* Inspect the S&P 500 Index Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-3R13wKK1rz"
      },
      "source": [
        "path = 'SPY.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhcfLdSHm6p-"
      },
      "source": [
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvddv8czJDuD"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2pjjH-GEX9C"
      },
      "source": [
        "plt.plot(df.Date, df.Close)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bguM7haxU3PL"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cz4C73TxG_Y"
      },
      "source": [
        "train_size = int(len(df) * 0.8)\n",
        "test_size = len(df) - train_size\n",
        "train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
        "print(train.shape, test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkxx7ETZ8oF8"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(train[['Close']])\n",
        "\n",
        "train['Close'] = scaler.transform(train[['Close']])\n",
        "test['Close'] = scaler.transform(test[['Close']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcGkRjsLfZxV"
      },
      "source": [
        "### Create time series data\n",
        "\n",
        "Select the variable (column) from the data and create the series of data with a window size.\n",
        "\n",
        "Refer [LSTM Autoencoder](https://medium.com/swlh/time-series-anomaly-detection-with-lstm-autoencoders-7bac1305e713)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp89-jBZpskM"
      },
      "source": [
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        v = X.iloc[i:(i + time_steps)].values#.reshape(-1)\n",
        "        Xs.append(v)\n",
        "        ys.append(y.iloc[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFpUz7OFps8J"
      },
      "source": [
        "time_steps = 30\n",
        "\n",
        "X_train, y_train = create_dataset(train[['Close']], train.Close, time_steps)\n",
        "X_test, y_test = create_dataset(test[['Close']], test.Close, time_steps)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWXRfhaMfdI5"
      },
      "source": [
        "### Build an LSTM Autoencoder\n",
        "\n",
        "Autoencoder should take a sequence as input and outputs a sequence of the same shape.\n",
        "\n",
        "Hint: [LSTM Autoencoder](https://medium.com/swlh/time-series-anomaly-detection-with-lstm-autoencoders-7bac1305e713)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FTdbOj8ZP9G"
      },
      "source": [
        "timesteps = X_train.shape[1]\n",
        "num_features = X_train.shape[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da55zoY1qSQ0"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(128, input_shape=(timesteps, num_features)),\n",
        "    Dropout(0.2),\n",
        "    RepeatVector(timesteps),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    TimeDistributed(Dense(num_features))\n",
        "])\n",
        "\n",
        "model.compile(loss='mae', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EvQrSQXLs3-"
      },
      "source": [
        "# # Create encoder submodel\n",
        "# encoder = Sequential([Dense(32, activation='relu', input_shape=[30]),\n",
        "#                       Dense(16, activation='relu'),\n",
        "#                       Dense(8, activation='relu')\n",
        "#                       ])\n",
        "\n",
        "# # Create decoder submodel\n",
        "# decoder = Sequential([Dense(16, activation='relu', input_shape=[8]),\n",
        "#                       Dense(32, activation='relu'),\n",
        "#                       Dense(30, activation='sigmoid')\n",
        "#                       ])\n",
        "\n",
        "# # Create autoencoder\n",
        "# autoencoder = Sequential([encoder, decoder])\n",
        "# autoencoder.compile(optimizer='adam', loss='mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL8k_JAjfhLt"
      },
      "source": [
        "### Train the Autoencoder\n",
        "\n",
        "* Compile and fit the model with required parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBqAR3SAapfJ"
      },
      "source": [
        "model.compile(loss='mae', optimizer='adam')\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1,\n",
        "                    shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW93bYQXcf2I"
      },
      "source": [
        "### Plot Metrics and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GYE03C1TUv0"
      },
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgOzzuc_xq8i"
      },
      "source": [
        "X_train_pred = model.predict(X_train)\n",
        "train_mae_loss = pd.DataFrame(np.mean(np.abs(X_train_pred - X_train), axis=1), columns=['Error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZX9-6ahVKY4"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coepllErx5Il"
      },
      "source": [
        "sns.distplot(train_mae_loss, bins=50, kde=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0IRFw03cP2V"
      },
      "source": [
        "X_test_pred = model.predict(X_test)\n",
        "test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z3-49OkWu20"
      },
      "source": [
        "sns.distplot(test_mae_loss, bins=50, kde=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C8bQAqZcqiw"
      },
      "source": [
        "### Detect Anomalies in the S&P 500 Index Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-87Uma2uEg6"
      },
      "source": [
        "THRESHOLD = 0.65\n",
        "\n",
        "test_score_df = pd.DataFrame(test[time_steps:])\n",
        "test_score_df['loss'] = test_mae_loss\n",
        "test_score_df['threshold'] = THRESHOLD\n",
        "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
        "test_score_df['Close'] = test[time_steps:].Close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkVxvoAt0Egc"
      },
      "source": [
        "anomalies = test_score_df[test_score_df.anomaly == True]\n",
        "anomalies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r5yjtuiYq0s"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Z9OEy9ewIm"
      },
      "source": [
        "!pip -qq install yfinance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxofxUZ_Tf9M"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPOkq_rSThvy"
      },
      "source": [
        "spy_ohlc_df = yf.download('SPY', start='1993-02-01', end='2021-06-01')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hac080xTmq-"
      },
      "source": [
        "spy_ohlc_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuPagyMhTo9D"
      },
      "source": [
        "spy_ohlc_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EqOWf4vTqhW"
      },
      "source": [
        "spy_ohlc_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6EM4Yy6T4EG"
      },
      "source": [
        "spy_ohlc_df.reset_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nkEjsoNT9DT"
      },
      "source": [
        "#spy_ohlc_df.to_csv(\"SPY.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lApsMwfXUAu5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}