{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Housing Case Study - End to End ML Process\n",
        "\n",
        "**Business Need** - Make real-estate investment decision for a large corporation.\n",
        "\n",
        "**Sub-Problem** - Predict the median house price in a block (smallest census group in US) given other demographic and geographic information.\n",
        "\n",
        "**For use in** - Other downstream models that need median house price in a block. For example, they may consider a greenfield project. So they need to know what a new block would be.\n",
        "\n",
        "**Data Science Problem** - Predict a value (the median house price in a block) given several features (other demographic and geographic information.)\n",
        "\n",
        "It is a supervised learning problem.\n",
        "\n",
        "**Dataset** - California Census Data with Median Housing Price"
      ],
      "metadata": {
        "id": "pgAK82EC_AGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case Study Prepared by modifying Chapter 2 of Geron's textbook by Prof. Deepak Subramani, CDS\n",
        "\n",
        "Instructions:\n",
        "* Please go through the code, most of the blocks are self explanatory.\n",
        "* You will gain more practice with each part through the course.\n",
        "* Don't worry even if you don;t understand everything on day 1.\n",
        "* Your goal should be to execute a project like this at the end of Module 2."
      ],
      "metadata": {
        "id": "oHnxtsOWra8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic Imports, Plot Beautification\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "mpl.rc('axes',labelsize=14)\n",
        "mpl.rc('xtick',labelsize=12)\n",
        "mpl.rc('ytick',labelsize=12)"
      ],
      "metadata": {
        "id": "oac3rbPfVI6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup Figure Saving\n",
        "\n",
        "fig_dir_path = os.path.join(\".\",\"figs\")\n",
        "os.makedirs(fig_dir_path, exist_ok=True)\n",
        "\n",
        "def save_this(fig_id, tight_layout=True, fig_extension = \"png\", resolution = 300):\n",
        "  fig_full_path = os.path.join(fig_dir_path,fig_id+'.'+fig_extension)\n",
        "  print(\"saving figure\",fig_id)\n",
        "  if tight_layout:\n",
        "    plt.tight_layout()\n",
        "  plt.savefig(fig_full_path,dpi=resolution,format=fig_extension)\n"
      ],
      "metadata": {
        "id": "c7vGkX5OV1yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data#"
      ],
      "metadata": {
        "id": "tDiayAHp3GZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Download data from the following link into your local system: https://www.dropbox.com/s/hwnd5h0f0in0jt0/housing.csv?dl=0\n",
        "\n",
        "  - Upload the file from your local system to this notebook by clicking the 'Upload to session storage' icon in the left panel\n",
        "\n",
        "  - Wait for the file to be 100% uploaded.\n",
        "\n",
        "  - You will be able to see the name of the file in the left panel once it is uploaded into Colab\n",
        "\n",
        "  - The following code will import it (Make sure the filename matches the name of the uploaded file).\n"
      ],
      "metadata": {
        "id": "v_4DVfgxpcgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "6wcOYXz93BRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "ls             #lists the directory files\n"
      ],
      "metadata": {
        "id": "ob6mKebfi_ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Familiarize yourself with the tabular data**\n",
        "\n",
        "Load data from CSV to a dataframe and look at the first 5 rows.\n",
        "\n",
        "Look at what types are these columns (find their info)\n",
        "\n"
      ],
      "metadata": {
        "id": "OZkKYu7lrPM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io                                              #io is a set of top level reader functions accessed like pandas .read_csv() that generally returns a pandas object\n",
        "df = pd.read_csv(io.BytesIO(uploaded['housing.csv']))  #BytesIO are methods that manipulate bytes data in memory which is used for binary data\n",
        "df.head()  #To see first 5 records"
      ],
      "metadata": {
        "id": "N7faa-9_4EcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Find the number of rows and columns of the data frame.\n",
        "\n",
        "Task: Read the column names and try to understand what they mean. If you don't understand, ask the upstream team who provided the data. Ask the data engineer who created the pipeline.\n",
        "\n",
        "For this case study, we will provide you the meaning of each column:\n",
        "\n",
        "longitude, latitute: Geographical location of each block (\"district\")\n",
        "\n",
        "population: Total Population in that block.\n",
        "\n",
        "households: Total Households in that block.\n",
        "\n",
        "housing_mean_age: Median age of the households in that block.\n",
        "\n",
        "total_room: Total number of rooms in all the houses in that block.\n",
        "\n",
        "total_bedroomns: Total number of bedrooms in all the houses in that block.\n",
        "\n",
        "median_income: Median income of the households in that block.\n",
        "\n",
        "median_house_value: Median value of houses in that block (this is our Target)\n",
        "\n",
        "ocean_proximity: Categorical variable describing the proximity of that block to the ocean."
      ],
      "metadata": {
        "id": "s_fkG3pm6hMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape     # To check number of rows and columns in the data"
      ],
      "metadata": {
        "id": "SK5QiZFH5Py7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "th0ZEzGHFYT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that Columns 0-8 are float64 and 9 is an object."
      ],
      "metadata": {
        "id": "B8EflPbmGTr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory Data Analysis#"
      ],
      "metadata": {
        "id": "EiUUtOtcE48L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Find the number of unique items in the column \"ocean_proximity\""
      ],
      "metadata": {
        "id": "7t_kJ7i2Fwg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ocean_proximity\"].value_counts()    #value_counts returns the count of the unique values of the perticular column"
      ],
      "metadata": {
        "id": "n63Tl9RvE9Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ocean_proximity\"].unique()   # returns the unique values of series object"
      ],
      "metadata": {
        "id": "d8eIUyoy6buZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Find the descriptive statistics of the numerical values in the table"
      ],
      "metadata": {
        "id": "7NhsbdfdG8xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe() #Finds the mean, std, min, quartiles, max of all numerical values"
      ],
      "metadata": {
        "id": "BjOGMZi9GVUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have explored both numerical and categorical variables and got an understanding of the scale of the data\n",
        "\n",
        "There are some issues to notice:\n",
        "\n",
        "\n",
        "\n",
        "1.   Is median_income strange?\n",
        "2.   Why do we have total_bedrooms and total_rooms? Won't bedrooms per house or rooms per house be more useful?\n",
        "\n",
        "3.   How to deal with Categorical Data?\n",
        "\n",
        "4. Are longitude and latitude useful?"
      ],
      "metadata": {
        "id": "Rx3nX9xoHKL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, median_income is strange. First it seems to be not in simple dollars. Surely, income is not 3.87 dollars on average. And there seems to be a max of 15.0001. You have to enquire with the data provider on why this is the case.\n",
        "\n",
        "On asking, the upstream team tells you that it is actually in 10k USD and there is an upper cut-off of 150k USD, meaning median_income is never above 15k USD in this table.\n",
        "\n",
        "You immediately notice that median_house_value is also bounded at 500001. So you enquire and understand that median_house_value is indeed in USD and an upper bound is set.\n",
        "\n",
        "This latter finding is problematic for you. median_house_value is our Target, and the training data has artifically imposed a limit on our target. It is a warning sign to be careful.\n",
        "\n",
        "Now let us look at the distribution of all the numerical values."
      ],
      "metadata": {
        "id": "mNj9ZkYbINoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Beautify the histogram plot. Show it in a readable format. Play around with the bin sizes and get it into an interpretable format."
      ],
      "metadata": {
        "id": "IMNaKN45LW2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist(bins=25,figsize=(20,15))   # hist funtion gives the distribution of the numeric variables"
      ],
      "metadata": {
        "id": "lRtNU82_LRpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_this(\"df_value_histogram\")"
      ],
      "metadata": {
        "id": "r9u5csSdMGdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpret**\n",
        "\n",
        "Histogram of longitude and latitude show that most blocks are concentrated in a few locations. We need geographical plots to see where these locations are.\n",
        "\n",
        "Median Housing Age seems to have a upper limit as well at 52. Is it problematic?\n",
        "\n",
        "Median House Value has a chunk at the upper limit. This can be a problem. One option is to remove it completely from the dataset, and clarify to downstream teams that our system cannot predict median_house_value beyond 500000 USD.\n",
        "\n",
        "Here, we retain and proceed."
      ],
      "metadata": {
        "id": "eCEVdw1PL8ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train-Test Split\n",
        "\n",
        "Before we proceed further we must split the dataset into a train (really the train-validate) and test set. The test set is then usually removed from the working environment and saved separately.\n",
        "\n",
        "We looked at the data attributes and histogram before Train-Test Split is to ensure that our dataset is representative of all targets.\n",
        "\n",
        "From our above analysis, we do not suspect any non-representativeness (except for >500k USD house price).\n",
        "\n",
        "Let us do a simple test-train split using sklearn."
      ],
      "metadata": {
        "id": "ZKU5lD-msH5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "elEQIdAKLpBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_test_split can work on X, y as well. If the input is X,y instead of df, then train_test_split returns X_train, X_test, y_train and y_test.\n",
        "\n",
        "The above splitting is usually enough. But in special cases, we employ what is called as \"Stratified Shuffle Split\".\n",
        "\n",
        "Let us say that you did train_test_split as above, and your upstream data colleague invited you for 11 am coffee. At the coffee shop he tells you, \"Pay attention to median_income. I think that median_income is extremely important for predicting median_housing_value.\"\n",
        "\n",
        "So you come back and decide to explore median_income more."
      ],
      "metadata": {
        "id": "pZQrsEIkyBIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"median_income\"].hist(bins=50)"
      ],
      "metadata": {
        "id": "PM9bdeCpw9h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"median_income\"].describe()"
      ],
      "metadata": {
        "id": "8dLtiXW5x-pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You see that bulk of the data is between 1.5 (15k) and 6 (60k). This also confirms your understanding of median incomes. So you decide to create bins and categorize income to marginal (0-1.5), low (1.5-3), middle (3-4.5), upper middle (4.5-6), upper classes (6+).\n",
        "\n",
        "Task: Use pandas function pd.cut and create the above categorization.\n",
        "\n",
        "Q: What is the count of each category?\n",
        "\n"
      ],
      "metadata": {
        "id": "8aVrJa2CzIAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"income_cat\"] = pd.cut(df[\"median_income\"],bins=[0., 1.5, 3., 4.5, 6., np.inf ], labels=[1,2,3,4,5])"
      ],
      "metadata": {
        "id": "Rb_h_tuszAac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"income_cat\"].value_counts()"
      ],
      "metadata": {
        "id": "G7BmRmZL0aOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have 5 strata in the dataset. Ideally, we want the train_test_split to sample in the specified ratio from each strata. For this, we use sklearn's stratify keyword arg of train_test_split.\n",
        "\n",
        "Note: Earlier we said that dealing with categorical variables is hard. But now we converted a continuos value to a categorical variable. This conversion helps us in the present application to perform stratifies sampling for train_test_split."
      ],
      "metadata": {
        "id": "IGy9E0li0svS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42) #No stratification\n",
        "df_train_strat, df_test_strat = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"income_cat\"])"
      ],
      "metadata": {
        "id": "eIv4ZKPA0gbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see the difference in value_counts proportions in df_train and df_train_strat"
      ],
      "metadata": {
        "id": "FkVMYu_93Fru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def income_cat_proportions(data):\n",
        "    return data[\"income_cat\"].value_counts() / len(data)\n",
        "\n",
        "compare_props_train = pd.DataFrame({\n",
        "    \"Overall\": income_cat_proportions(df),\n",
        "    \"Stratified\": income_cat_proportions(df_train_strat),\n",
        "    \"Random\": income_cat_proportions(df_train),\n",
        "}).sort_index()\n",
        "compare_props_train[\"Rand. %error\"] = 100 * compare_props_train[\"Random\"] / compare_props_train[\"Overall\"] - 100\n",
        "compare_props_train[\"Strat. %error\"] = 100 * compare_props_train[\"Stratified\"] / compare_props_train[\"Overall\"] - 100"
      ],
      "metadata": {
        "id": "a3rhMlIx2CwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_props_train"
      ],
      "metadata": {
        "id": "tX2rsnDK3WqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, ideally we must remove df_test_strat from the notebook and save it as a test.csv.\n",
        "\n",
        "This is an precaution to avoid any kind of accidental data leakage.\n",
        "\n",
        "If you follow the non-notebook mechanism (as in AAMPL), a separate cross-validation split is used and train and test csv are built as a first step.\n",
        "\n",
        "Here, we trust that we will be careful in avoiding data leakage (i.e., test set is somehow used during train-validate)."
      ],
      "metadata": {
        "id": "-bRMJi_6_m_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore train data for ML modeling"
      ],
      "metadata": {
        "id": "YRn6vDaKAOzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_strat.drop(\"income_cat\", axis=1, inplace=True) # income_cat was created for stratified split. Its use is over. Let us remove it.\n",
        "df_test_strat.drop(\"income_cat\", axis=1, inplace=True)\n",
        "housing = df_train_strat.copy() # Create a copy to avoid data leakage, and accidental deletion."
      ],
      "metadata": {
        "id": "S4Kcebbk3cgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geo Plots**\n",
        "\n",
        "Since this is a geographical dataset, we can plot the target on a map to visualize. Such visualization can sometimes trigger ideas on what features to use.\n",
        "\n",
        "We do not plot it here. But you can explore geographical plotting with several resources\n",
        "\n",
        "\n",
        "*   https://jakevdp.github.io/PythonDataScienceHandbook/04.13-geographic-data-with-basemap.html\n",
        "*   https://geopandas.org/en/stable/docs/user_guide/mapping.html\n",
        "*   https://towardsdatascience.com/plotting-maps-with-geopandas-428c97295a73\n",
        "\n"
      ],
      "metadata": {
        "id": "_wlnwAmNDRSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation**\n",
        "\n",
        "Let us see the correlation of all the features with the target"
      ],
      "metadata": {
        "id": "aWLIw8OOPwKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_mat = housing.corr()\n",
        "corr_mat[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "9auQ2rD-S6f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the heat map of the correlation matrix"
      ],
      "metadata": {
        "id": "jGnnf-LSGZj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(corr_mat)"
      ],
      "metadata": {
        "id": "MqqGKuI1GfDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_mat"
      ],
      "metadata": {
        "id": "7RPC15EdE92_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that median_income has the highest correlation with median_house_value as our friend had suggested. We already did a smart move - of stratified sampling.\n",
        "\n",
        "We also note that latitude has a negative correlation with house value. Its implication is that moving north reduces the house value.\n",
        "\n",
        "Population has a slightly negative correlation as expected.\n",
        "\n",
        "Let us look at a few more visualizations of the data\n",
        "\n",
        "Task 1: Plot the scatter of median_house_vale, median_income, total_rooms, housing_median_age using seaborn"
      ],
      "metadata": {
        "id": "l4b0_iWITToq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=housing, x=\"median_income\", y=\"total_rooms\", hue=\"median_house_value\")"
      ],
      "metadata": {
        "id": "D3zk1xCXXSw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_needed = [\"median_income\",\"total_rooms\",\"housing_median_age\",\"median_house_value\"]\n",
        "sns.pairplot(data=housing[cols_needed], hue=\"median_house_value\") #This will take some time"
      ],
      "metadata": {
        "id": "8fgTY6nfXpWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This phase is to fire your brain to think of possible combinations for feature engieering. You must use your business knowledge (or process, science, domain knowledge) to figure out what features are important for this problem."
      ],
      "metadata": {
        "id": "9qTGebyuYnkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering**\n",
        "\n",
        "The strange issue here is that total_rooms and total_bedrooms are not as correlated with median_house_value. In fact, our intutition suggests that 3BHK is more pricier than 2BHK etc. So we should look at rooms and bedroom per household.\n",
        "\n",
        "Let us do that feature engineering.\n",
        "\n",
        "Task: Add new features of rooms_per_house and bedrooms_per_house\n",
        "\n",
        "Q: What is the correlation of the new features to the target."
      ],
      "metadata": {
        "id": "Mq5EZ_efWldc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing[\"rooms_per_house\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
        "housing[\"bedrooms_per_house\"] = housing[\"total_bedrooms\"]/housing[\"households\"]\n",
        "corr_mat2 = housing.corr()\n",
        "corr_mat2[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "TTKA357LVWtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not much changed. Let us try different combinations of rooms.\n",
        "\n",
        "\n",
        "*   bedrooms_per_room\n",
        "*   population_per_house\n",
        "\n"
      ],
      "metadata": {
        "id": "C-5alur9WA9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
        "housing[\"population_per_house\"] = housing[\"population\"]/housing[\"households\"]\n",
        "corr_mat3 = housing.corr()\n",
        "corr_mat3[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "syX-5rJBZhGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK! Bedrooms_per_room is more correlated (-vely) with house value. Apparently houses with more bedrooms cost less. Somehow people seem to value houses with more non-bedrooms here.\n",
        "\n",
        "Message: We have identified that median_income, bedrooms_per_room and rooms_per_house (or total_rooms), housing_median_age seem to be good predictors for house_value. Latitude is also correlated, but it seems to be strange to include that as a predictor.\n",
        "\n",
        "We still have to worry about missing data and handling categorical data.\n",
        "\n",
        "Now we are entering the stage of preparing the data for ML.\n",
        "\n",
        "Remember we need X and y for ML."
      ],
      "metadata": {
        "id": "9nZ2YgLeZ6KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data for ML"
      ],
      "metadata": {
        "id": "DpPVzmNda3xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hml = df_train_strat.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
        "hml_labels = df_train_strat[\"median_house_value\"].copy()\n",
        "# Note: Here we did not use the housing df as above. We have modified housing df for exploration purposes. We will use that learning in creating an automatic data processing pipeline soon."
      ],
      "metadata": {
        "id": "VS6WsokCRW-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle Missing Data**\n",
        "\n",
        "Q: How many data points are missing? What featues are missing?"
      ],
      "metadata": {
        "id": "bs9JyIyNRYDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hml.info()"
      ],
      "metadata": {
        "id": "b3WLeJvecfGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "total_bedrooms has some missing values. We have to handle this.\n",
        "\n",
        "Missing Values are handled using an imputer.\n",
        "\n",
        "Imputation is a statistical process of replacing null values (missing values) with statistics of the available values. In advanced applications, we even use another ML model (say regression, or RF) as an imputer. See: https://en.wikipedia.org/wiki/Imputation_(statistics)\n",
        "\n",
        "In this particular case, the number of missing entries are small, and only the total_bedrooms feature is missing. So either we can drop the rows where the total_bedrooms is missing; or we can drop the feature total_bedrooms.\n",
        "\n",
        "If we don't want to lose valuable datapoints, typically, we use replacement with mean or median as an imputer. Here, we will simply use median and sklearn.impute.SimpleImputer\n",
        "\n",
        "SimpleImputer with median only works with numerical data.\n",
        "\n",
        "So we will only use numerical data to perform the imuptation"
      ],
      "metadata": {
        "id": "7EhGeodCdBJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "hml_num = hml.drop(\"ocean_proximity\", axis=1)\n",
        "\n",
        "imputer.fit(hml_num)\n",
        "\n",
        "X = imputer.transform(hml_num) # Imputer returns a numpy array. So we need to transform it back to a pandas df\n",
        "\n",
        "hml_num_tr = pd.DataFrame(data=X,columns=hml_num.columns,index=hml_num.index)"
      ],
      "metadata": {
        "id": "vjnocvbMdyHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hml_num_tr.info()"
      ],
      "metadata": {
        "id": "l3Kg5IHyjiSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, so good. We have successfully removed the nan entries. You can verify if everything went correct by looking at the rows with nan entries in hml and corresponding rows in hml_num_tr.\n",
        "\n",
        "Before we deal with categorical data, let us add the features we engineered."
      ],
      "metadata": {
        "id": "eDTSEMSXkDAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hml_num_tr[\"bedrooms_per_room\"] = hml_num_tr[\"total_bedrooms\"]/hml_num_tr[\"total_rooms\"]\n",
        "hml_num_tr[\"bedrooms_per_house\"] = hml_num_tr[\"total_bedrooms\"]/hml_num_tr[\"households\"]\n",
        "hml_num_tr[\"rooms_per_house\"] = hml_num_tr[\"total_rooms\"]/hml_num_tr[\"households\"]"
      ],
      "metadata": {
        "id": "nHVyzLsNor1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle Categorical Data**\n",
        "\n",
        "Ocean Proximity is a categorical variable. It must be handled.\n",
        "\n",
        "There are several ways to handle catergorical data. We will use a popular technique called as OneHotEncoder.\n",
        "\n",
        "We can use sklearn.preprocessing.OneHotEncoder\n",
        "\n",
        "Alternatively we can use pandas.get_dummies. See: https://towardsdatascience.com/what-is-one-hot-encoding-and-how-to-use-pandas-get-dummies-function-922eb9bd4970"
      ],
      "metadata": {
        "id": "YpQxUCPUP5dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "\n",
        "hml_cat = hml[[\"ocean_proximity\"]] # Use two square brackets as fit_transform expects a df. With singe square bracket, a series is returned\n",
        "\n",
        "hml_cat_1hot = cat_encoder.fit_transform(hml_cat)"
      ],
      "metadata": {
        "id": "-RiAXYiGlvp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hml_cat_1hot\n",
        "\n",
        "#this is a sparse array. We must use .toarray() to convert into a full matrix that can be concatenated to the numerical features."
      ],
      "metadata": {
        "id": "RFthR4_d5Dwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.concatenate([hml_num_tr.to_numpy(),hml_cat_1hot.toarray()],axis=1)\n",
        "\n",
        "y_train = hml_labels.values"
      ],
      "metadata": {
        "id": "w0vPoX_n6FFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "pcpZQ0bQpT-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling**\n",
        "\n",
        "We observed that the features have different scales. So, we perform a standard scaling operation to bring all data to the same scale.\n",
        "\n",
        "Usually, targets don't need to be scaled. There is no harm in scaling the target separately, but we don't do it.\n",
        "\n",
        "Task: Use sklearn.preprocessing.StandardScaler"
      ],
      "metadata": {
        "id": "exHOKzgNsrA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scl = scaler.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "cX7JbuphtDFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this, we have the X_train_scl and y_train that can be used for machine learning.\n",
        "\n",
        "This X_train does not have the new features that we engineered.\n",
        "\n",
        "But, we have done a lot of work to transform our dataframe to features. We must do the same for future data, including test data.\n",
        "\n",
        "For this purpuse, we create a \"pipeline\" and use \"custom_transformation\""
      ],
      "metadata": {
        "id": "JZ_t329CnXcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Transformation and Pipeline"
      ],
      "metadata": {
        "id": "KwZVclq_rix7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Handle missing numerical data\n",
        "2. Add new features columns\n",
        "3. Handled the categorical data\n",
        "4. Scale"
      ],
      "metadata": {
        "id": "ZXnjrTcaWMeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First**: Create a custom transformation to add new attribute through feature combinations"
      ],
      "metadata": {
        "id": "5HQXFcgxzVe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "col_names = [\"total_rooms\",\"total_bedrooms\",\"population\",\"households\"]\n",
        "\n",
        "rooms_indx, bedrooms_indx, pop_indx, house_indx = [housing.columns.get_loc(ii) for ii in col_names] #We are using Python list comprehension\n",
        "\n",
        "class EngineeredFeatureAdder(BaseEstimator, TransformerMixin):\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  def transform(self, X):\n",
        "    rooms_per_household = X[:,rooms_indx]/X[:,house_indx]\n",
        "    bedrooms_per_household = X[:,bedrooms_indx]/X[:,house_indx]\n",
        "    population_per_household = X[:,pop_indx]/X[:,house_indx]\n",
        "    bedrooms_per_room = X[:,bedrooms_indx]/X[:,rooms_indx]\n",
        "    return np.c_[X,rooms_per_household,bedrooms_per_household,population_per_household,bedrooms_per_room]\n",
        "\n",
        "# Note: Here, we are adding all engineered features. This is probably not a good idea. We are simply showing an example."
      ],
      "metadata": {
        "id": "K4hAlr4itj81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input to the cusom transformer EngineeredFeatureAdder is a numpy array. We cannot pass in a pandas dataframe here.\n",
        "\n",
        "If we need a pandas dataframe later, then we have to create a pipeline with a pandasizer, a function that takes the output of the cusomt transformer and returns a pandas df.\n",
        "\n",
        "Exercise: Create a pipeline with a pandasizer"
      ],
      "metadata": {
        "id": "-I_3UoXl2z7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation Pipeline\n",
        "\n",
        "Build a pipeline for the numerical attributes, and categorical attributes separately.\n",
        "\n",
        "Then we will combine them together using column transformer\n",
        "\n",
        "We must impute and scale.\n",
        "\n"
      ],
      "metadata": {
        "id": "x5TL9YHA3AEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "                         ('imputer',SimpleImputer(strategy=\"median\")),\n",
        "                         ('feat_eng',EngineeredFeatureAdder()),\n",
        "                         ('scaler',StandardScaler())\n",
        "                         ])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "                         ('encoder',OneHotEncoder())\n",
        "                         ])\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_cols = list(hml_num) # Alternatively: hml_num.columns.values.tolist()\n",
        "cat_cols = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "                                   ('num',num_pipeline,num_cols),\n",
        "                                   ('cat',cat_pipeline,cat_cols)\n",
        "                                   ])\n",
        "\n",
        "hml_prepared = full_pipeline.fit_transform(hml)"
      ],
      "metadata": {
        "id": "zWasaLi80Ri6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WG89fSjy4SRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Modeling"
      ],
      "metadata": {
        "id": "Tjhipc7_rnGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "lin_reg.fit(hml_prepared,hml_labels)"
      ],
      "metadata": {
        "id": "GO5w3SIi6GPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "hml_test = df_test_strat.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
        "hml_test_labels = df_test_strat[\"median_house_value\"].copy()\n",
        "X_test = full_pipeline.transform(hml_test)\n",
        "y_test_predict = lin_reg.predict(X_test)\n",
        "mean_squared_error(hml_test_labels,y_test_predict,squared=False)\n"
      ],
      "metadata": {
        "id": "zFibF3VH66bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70pgRI8O7bjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(lin_reg,hml_prepared,hml_labels,scoring=\"neg_mean_squared_error\",cv=5)\n"
      ],
      "metadata": {
        "id": "Dfw-ORLv-bKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sqrt(-scores)"
      ],
      "metadata": {
        "id": "-HG71Rq_xLJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune your model\n",
        "\n",
        "The cross_val_score shows that the training is stable. The validation error is not varying widely.\n",
        "\n",
        "For linear regression, there is no further hyper-parameter tuning.\n",
        "\n",
        "But if we go for ElasticNet, or Random Forest, or XGBoost there is hyper-parameter tuning.\n",
        "\n",
        "Hyper-parameter tuning can be done with GridSearchCV or RandomizedSearchCV\n",
        "\n"
      ],
      "metadata": {
        "id": "oco1BmAE1Fmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "enet = ElasticNet()\n",
        "\n",
        "params = {'alpha': [0.1, 1, 10, 100], 'l1_ratio': [0.25,0.5,0.75]}\n",
        "\n",
        "regressor = GridSearchCV(enet,params,cv=5)\n",
        "\n",
        "regressor.fit(hml_prepared,hml_labels)"
      ],
      "metadata": {
        "id": "QjsmqX8KxRyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.best_params_"
      ],
      "metadata": {
        "id": "3FxtLl6F8JgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.cv_results_"
      ],
      "metadata": {
        "id": "K4y3zcFe8eeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best hyper-parameter set can be found from the CV results and used to train a best model.\n",
        "\n",
        "Setting the parameter refit=True will automatically do the refit for you.\n",
        "\n",
        "Or you can decide to pick the best three models and create an ensemble.\n",
        "\n",
        "We will study more about ensembles and voting in future lectures."
      ],
      "metadata": {
        "id": "Xox2y5648-A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = ElasticNet(alpha=0.1,l1_ratio=0.75)\n",
        "\n",
        "best_model.fit(hml_prepared,hml_labels)"
      ],
      "metadata": {
        "id": "hUwDKJ4w8mCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hml_test = df_test_strat.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
        "hml_test_labels = df_test_strat[\"median_house_value\"].copy()\n",
        "X_test = full_pipeline.transform(hml_test)\n",
        "y_test_predict = best_model.predict(X_test)\n",
        "mean_squared_error(hml_test_labels,y_test_predict,squared=False)\n"
      ],
      "metadata": {
        "id": "jnqJhl3Q9cnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The error is slightly on the higher side. An average error of 67k USD might not be acceptable for house values.\n",
        "\n",
        "Further modeling, feature engineering, and discussions with stakeholders is needed before finalizing a model for this study."
      ],
      "metadata": {
        "id": "EZ_AwdTL97mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "CtDlPG2o9nma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IpgI1-sgiydO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}